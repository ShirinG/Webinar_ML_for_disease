---
title: 'Webinar: Building meaningful machine learning models for disease prediction'
author: "Dr. Shirin Glander"
date: "March 31, 2017"
output:
  pdf_document:
    keep_tex: yes
  html_document: default
---

Webinar for ISDS R Group: http://www.syndromic.org/cop/r

Description: Dr Shirin Glander will go over her work on building machine-learning models to predict the course of different diseases. She will go over building a model, evaluating its performance, and answering or addressing different disease related questions using machine learning. Her talk will cover the theory of machine learning as it is applied using R.

\href{mailto:shirin.glander@wwu.de}{shirin.glander@wwu.de}
	
\href{https://shiring.github.io}{https://shiring.github.io}
	
\href{https://github.com/ShirinG}{https://github.com/ShirinG}

Slides and code will be available on Github: \href{https://github.com/ShirinG/Webinar_ML_for_disease}{https://github.com/ShirinG/Webinar\_ML\_for\_disease}
	
Code will also be on my website: \href{https://shiring.github.io}{https://shiring.github.io}

---

Can we predict flu deaths with Machine Learning and R?: https://shiring.github.io/machine_learning/2016/11/27/flu_outcome_ML_post
Extreme Gradient Boosting and Preprocessing in Machine Learning - Addendum to predicting flu outcome with R: https://shiring.github.io/machine_learning/2016/12/02/flu_outcome_ML_2_post
Feature Selection in Machine Learning (Breast Cancer Datasets): https://shiring.github.io/machine_learning/2017/01/15/rfe_ga_post
Predicting food preferences with sparklyr (machine learning): https://shiring.github.io/machine_learning/2017/02/19/food_spark
Building deep neural nets with h2o and rsparkling that predict arrhythmia of the heart: https://shiring.github.io/machine_learning/2017/02/27/h2o

```{r echo=FALSE, eval=FALSE}
# hypothetical example for describing meaningfulness

library(ggplot2)

df <- data.frame(actual = rep(c("healthy", "disease"), each = 2),
                 predicted = rep(c("healthy", "disease"), 2),
                 count = c(98, 2, 5, 95))

p1 <- ggplot(df, aes(x = predicted, y = count, fill = actual)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_bw() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Ideal case")

setwd("U:/Github_blog/Webinar/Webinar_ML_for_disease/images")

pdf("images/meaningful_1.pdf", onefile = TRUE, bg = "white", family = "Helvetica", width = 4, height = 3, useDingbats=F)
print(p1)
dev.off()

df <- data.frame(actual = rep(c("healthy", "disease"), each = 2),
                 predicted = rep(c("healthy", "disease"), 2),
                 count = c(60, 40, 5, 95))

p2 <- ggplot(df, aes(x = predicted, y = count, fill = actual)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_bw() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Possible case 1")

pdf("images/meaningful_2.pdf", onefile = TRUE, bg = "white", family = "Helvetica", width = 4, height = 3, useDingbats=F)
print(p2)
dev.off()

df <- data.frame(actual = rep(c("healthy", "disease"), each = 2),
                 predicted = rep(c("healthy", "disease"), 2),
                 count = c(90, 10, 35, 70))

p3 <- ggplot(df, aes(x = predicted, y = count, fill = actual)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_bw() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Possible case 2")

pdf("images/meaningful_3.pdf", onefile = TRUE, bg = "white", family = "Helvetica", width = 4, height = 3, useDingbats=F)
print(p3)
dev.off()
```


<br>

## Dataset

### Breast Cancer Wisconsin (Diagnostic) Dataset

The data was downloaded from the [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). The features in these datasets characterise cell nucleus properties and were generated from image analysis of [fine needle aspirates (FNA)](https://en.wikipedia.org/wiki/Fine-needle_aspiration) of breast masses. 

The first dataset looks at the predictor classes:

- malignant or
- benign breast mass.

The phenotypes for characterisation are:

- Sample ID (code number)
- Clump thickness
- Uniformity of cell size
- Uniformity of cell shape
- Marginal adhesion
- Single epithelial cell size
- Number of bare nuclei
- Bland chromatin
- Number of normal nuclei
- Mitosis
- Classes, i.e. diagnosis

Missing values are imputed with the *mice* package.

```{r eval=FALSE}
bc_data <- read.table("datasets/breast-cancer-wisconsin.data.txt", header = FALSE, sep = ",")
colnames(bc_data) <- c("sample_code_number", 
                       "clump_thickness", 
                       "uniformity_of_cell_size", 
                       "uniformity_of_cell_shape", 
                       "marginal_adhesion", 
                       "single_epithelial_cell_size", 
                       "bare_nuclei", 
                       "bland_chromatin", 
                       "normal_nucleoli", 
                       "mitosis", 
                       "classes")

bc_data$classes <- ifelse(bc_data$classes == "2", "benign",
                          ifelse(bc_data$classes == "4", "malignant", NA))

bc_data[bc_data == "?"] <- NA

# how many NAs are in the data
length(which(is.na(bc_data)))

# impute missing data
library(mice)

bc_data[,2:10] <- apply(bc_data[, 2:10], 2, function(x) as.numeric(as.character(x)))
dataset_impute <- mice(bc_data[, 2:10],  print = FALSE)
bc_data <- cbind(bc_data[, 11, drop = FALSE], mice::complete(dataset_impute, 1))

bc_data$classes <- as.factor(bc_data$classes)

# how many benign and malignant cases are there?
summary(bc_data$classes)
```

```{r echo=FALSE, eval=FALSE}
save(bc_data, file = "datasets/bc_data.RData")
```

```{r echo=FALSE}
load("datasets/bc_data.RData")
```

<br>

## Machine Learning packages for R

### caret

```{r warning=FALSE, message=FALSE}
library(caret)
```


#### Training, validation and test data

```{r}
set.seed(42)
index <- createDataPartition(bc_data$classes, p = 0.7, list = FALSE)
train_data <- bc_data[index, ]
test_data  <- bc_data[-index, ]
```

```{r fig.height=5, fig.width=20, fig.align="center", warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)

rbind(data.frame(group = "train", train_data),
                      data.frame(group = "test", test_data)) %>%
  gather(x, y, clump_thickness:mitosis) %>%
  ggplot(aes(x = y, color = group, fill = group)) +
    geom_density(alpha = 0.3) +
    facet_grid(classes ~ x, scales = "free")
```

#### Classification

##### Decision trees

```{r cache=TRUE, fig.height=5, fig.width=10, fig.align="center"}
library(rpart)
library(rpart.plot)

set.seed(42)
fit <- rpart(classes ~ .,
            data = train_data,
            method = "class",
            control = rpart.control(xval = 10, 
                                    minbucket = 2, 
                                    cp = 0), 
             parms = list(split = "information"))

rpart.plot(fit, extra = 100)
```

<br>

##### Random Forests

Can be used for classification and regression tasks. Here, I show a classification task.

```{r eval=FALSE}
set.seed(42)
model_rf <- caret::train(classes ~ .,
                         data = train_data,
                         method = "rf",
                         preProcess = c("scale", "center"),
                         trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))
```

When you specify `savePredictions = TRUE`, you can access the cross-validation resuls with `model_rf$pred`.

```{r echo=FALSE, eval=FALSE}
save(model_rf, file = "models/model_rf.RData")
```

```{r echo=FALSE}
load("models/model_rf.RData")
```

- Feature Importance

```{r warning=FALSE, message=FALSE}
# estimate variable importance
importance <- varImp(model_rf, scale = TRUE)

plot(importance)
```

- predicting test data

```{r}
confusionMatrix(predict(model_rf, test_data), test_data$classes)
```

<br>

##### Extreme gradient boosting trees

Can be used for classification and regression tasks. Here, I show a classification task.

```{r eval=FALSE}
set.seed(42)
model_xgb <- caret::train(classes ~ .,
                          data = train_data,
                          method = "xgbTree",
                          preProcess = c("scale", "center"),
                          trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))
```

```{r echo=FALSE, eval=FALSE}
save(model_xgb, file = "models/model_xgb.RData")
```

```{r echo=FALSE}
load("models/model_xgb.RData")
```

- Feature Importance

```{r warning=FALSE, message=FALSE}
# estimate variable importance
importance <- varImp(model_xgb, scale = TRUE)

plot(importance)
```

- predicting test data

```{r}
confusionMatrix(predict(model_xgb, test_data), test_data$classes)
```

<br>

#### Regression

```{r cache=TRUE}
set.seed(42)
model_glm <- caret::train(clump_thickness ~ .,
                          data = train_data,
                          method = "glm",
                          preProcess = c("scale", "center"),
                          trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))
```

```{r echo=FALSE, eval=FALSE}
save(model_glm, file = "models/model_glm.RData")
```

```{r echo=FALSE}
load("models/model_glm.RData")
```

```{r}
data.frame(actual = test_data$clump_thickness,
           predicted = predict(model_glm, test_data)) %>%
  ggplot(aes(x = actual, y = predicted)) +
    geom_jitter() +
    geom_smooth(method = "lm")
```

<br>

### Grid search with h2o

```{r warning=FALSE, message=FALSE}
library(h2o)
h2o.init()

bc_data_hf <- as.h2o(bc_data)
```

```{r warning=FALSE, message=FALSE, fig.width=6, fig.height=6, fig.align='center', tidy=FALSE}
library(tidyr)

h2o.describe(bc_data_hf) %>%
  gather(x, y, Zeros:Sigma) %>%
  mutate(group = ifelse(x %in% c("Min", "Max", "Mean"), "min, mean, max", 
                        ifelse(x %in% c("NegInf", "PosInf"), "Inf", "sigma, zeros"))) %>% 
  ggplot(aes(x = Label, y = as.numeric(y), color = x)) +
    geom_point(size = 4, alpha = 0.6) +
    scale_color_brewer(palette = "Set1") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    facet_grid(group ~ ., scales = "free") +
    labs(x = "Feature",
         y = "Value",
         color = "")
```

<br>

#### Training, validation and test data

```{r tidy=FALSE}
splits <- h2o.splitFrame(bc_data_hf, 
                         ratios = c(0.7, 0.15), 
                         seed = 1)

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

response <- "classes"
features <- setdiff(colnames(train), response)
```

```{r tidy=FALSE}
summary(train$classes, exact_quantiles = TRUE)
summary(valid$classes, exact_quantiles = TRUE)
summary(test$classes, exact_quantiles = TRUE)
```

<br>

#### Classification

##### Random Forest

Can be used for classification and regression tasks. Here, I show a classification task.

```{r eval=FALSE}
hyper_params <- list(
                     ntrees = c(25, 50, 75, 100),
                     max_depth = c(10, 20, 30),
                     min_rows = c(1, 3, 5)
                     )

search_criteria <- list(
                        strategy = "RandomDiscrete", 
                        max_models = 50,
                        max_runtime_secs = 360,
                        stopping_rounds = 5,          
                        stopping_metric = "AUC",      
                        stopping_tolerance = 0.0005,
                        seed = 42
                        )
```

```{r eval=FALSE}
rf_grid <- h2o.grid(algorithm = "randomForest", # h2o.randomForest, 
                                                # alternatively h2o.gbm for Gradient boosting trees
                    x = features,
                    y = response,
                    grid_id = "rf_grid",
                    training_frame = train,
                    validation_frame = valid,
                    nfolds = 25,                           
                    fold_assignment = "Stratified",
                    hyper_params = hyper_params,
                    search_criteria = search_criteria,
                    seed = 42
                    )
```

```{r eval=FALSE}
# performance metrics where smaller is better -> order with decreasing = FALSE
sort_options_1 <- c("mean_per_class_error", "mse", "err", "logloss")

for (sort_by_1 in sort_options_1) {
  
  grid <- h2o.getGrid("rf_grid", sort_by = sort_by_1, decreasing = FALSE)
  
  model_ids <- grid@model_ids
  best_model <- h2o.getModel(model_ids[[1]])
  
  h2o.saveModel(best_model, path="models", force = TRUE)
  
}


# performance metrics where bigger is better -> order with decreasing = TRUE
sort_options_2 <- c("auc", "precision", "accuracy", "recall", "specificity")

for (sort_by_2 in sort_options_2) {
  
  grid <- h2o.getGrid("rf_grid", sort_by = sort_by_2, decreasing = TRUE)
  
  model_ids <- grid@model_ids
  best_model <- h2o.getModel(model_ids[[1]])
  
  h2o.saveModel(best_model, path="models", force = TRUE)
  
}
```

```{r fig.height=5, fig.width=8}
files <- list.files(path = "models")
rf_models <- files[grep("rf_grid_model", files)]

for (model_id in rf_models) {
  
  path <- paste0("U:\\Github_blog\\Webinar\\Webinar_ML_for_disease\\models\\", model_id)
  best_model <- h2o.loadModel(path)
  mse_auc_test <- data.frame(model_id = model_id, 
                             mse = h2o.mse(h2o.performance(best_model, test)),
                             auc = h2o.auc(h2o.performance(best_model, test)))
  
  if (model_id == rf_models[[1]]) {
    
    mse_auc_test_comb <- mse_auc_test
    
  } else {
    
    mse_auc_test_comb <- rbind(mse_auc_test_comb, mse_auc_test)
    
  }
}

mse_auc_test_comb %>%
  gather(x, y, mse:auc) %>%
  ggplot(aes(x = model_id, y = y, fill = model_id)) +
    facet_grid(x ~ ., scales = "free") +
    geom_bar(stat = "identity", alpha = 0.8, position = "dodge") +
    scale_fill_brewer(palette = "Set1") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),
          plot.margin = unit(c(0.5, 0, 0, 1.5), "cm")) +
    labs(x = "", y = "value", fill = "")
```

```{r fig.height=4, fig.width=8, warning=FALSE, message=FALSE}
for (model_id in rf_models) {
  
  best_model <- h2o.getModel(model_id)
  
  finalRf_predictions <- data.frame(model_id = rep(best_model@model_id, 
                                                   nrow(test)),
                                    actual = as.vector(test$classes), 
                                    as.data.frame(h2o.predict(object = best_model, 
                                                              newdata = test)))
  
  finalRf_predictions$accurate <- ifelse(finalRf_predictions$actual == finalRf_predictions$predict, 
                                         "yes", "no")
  
  finalRf_predictions$predict_stringent <- ifelse(finalRf_predictions$benign > 0.8, 
                                                  "benign", 
                                                  ifelse(finalRf_predictions$malignant > 0.8, 
                                                         "malignant", "uncertain"))
  
  finalRf_predictions$accurate_stringent <- ifelse(finalRf_predictions$actual == finalRf_predictions$predict_stringent, 
                                                   "yes", 
                                         ifelse(finalRf_predictions$predict_stringent == "uncertain", 
                                                "na", "no"))
  
  if (model_id == rf_models[[1]]) {
    
    finalRf_predictions_comb <- finalRf_predictions
    
  } else {
    
    finalRf_predictions_comb <- rbind(finalRf_predictions_comb, finalRf_predictions)
    
  }
}
```

```{r fig.height=3, fig.width=9}
finalRf_predictions_comb %>%
  ggplot(aes(x = actual, fill = accurate)) +
    geom_bar(position = "dodge") +
    scale_fill_brewer(palette = "Set1") +
    facet_wrap(~ model_id, ncol = 3) +
    labs(fill = "Were\npredictions\naccurate?",
         title = "Default predictions")

finalRf_predictions_comb %>%
  subset(accurate_stringent != "na") %>%
  ggplot(aes(x = actual, fill = accurate_stringent)) +
    geom_bar(position = "dodge") +
    scale_fill_brewer(palette = "Set1") +
    facet_wrap(~ model_id, ncol = 3) +
    labs(fill = "Were\npredictions\naccurate?",
         title = "Stringent predictions")
```

```{r }
rf_model <- h2o.loadModel("U:\\Github_blog\\Webinar\\Webinar_ML_for_disease\\models\\rf_grid_model_6")
rf_model
```

```{r }
h2o.varimp_plot(rf_model)
#h2o.varimp(rf_model)
```

One performance metric we are interested in is the mean per class error for training and validation data.

```{r }
h2o.mean_per_class_error(rf_model, train = TRUE, valid = TRUE, xval = TRUE)
```

The confusion matrix tells us, how many classes have been predicted correctly and how many predictions were accurate. Here, we see the errors in predictions on validation data

```{r }
h2o.confusionMatrix(rf_model, valid = TRUE)
```

We can also plot the classification error.

```{r fig.width=8, fig.height=4, fig.align='center'}
plot(rf_model,
     timestep = "number_of_trees",
     metric = "classification_error")
```

Next to the classification error, we are usually interested in the logistic loss (negative log-likelihood or log loss). It describes the sum of errors for each sample in the training or validation data or the negative logarithm of the likelihood of error for a given prediction/ classification. Simply put, the lower the loss, the better the model (if we ignore potential overfitting).

```{r fig.width=8, fig.height=4, fig.align='center'}
plot(rf_model,
     timestep = "number_of_trees",
     metric = "logloss")
```

```{r fig.width=8, fig.height=4, fig.align='center'}
plot(rf_model,
     timestep = "number_of_trees",
     metric = "AUC")
```

We can also plot the mean squared error (MSE). The MSE tells us the average of the prediction errors squared, i.e. the estimator's variance and bias. The closer to zero, the better a model.

```{r fig.width=8, fig.height=4, fig.align='center'}
plot(rf_model,
     timestep = "number_of_trees",
     metric = "rmse")
```

Next, we want to know the area under the curve (AUC). AUC is an important metric for measuring binary classification model performances. It gives the area under the curve, i.e. the integral, of true positive vs false positive rates. The closer to 1, the better a model. AUC is especially useful, when we have unbalanced datasets (meaning datasets where one class is much more common than the other), because it is independent of class labels.

```{r }
h2o.auc(rf_model, train = TRUE)
h2o.auc(rf_model, valid = TRUE)
h2o.auc(rf_model, xval = TRUE)
```

Now that we have a good idea about model performance on validation data, we want to know how it performed on unseen test data. A good model should find an optimal balance between accuracy on training and test data. A model that has 0% error on the training data but 40% error on the test data is in effect useless. It overfit on the training data and is thus not able to generalize to unknown data.

```{r fig.width=6, fig.height=5, fig.align='center'}
perf <- h2o.performance(rf_model, test)
perf
```

Plotting the test performance's AUC plot shows us approximately how good the predictions are.

```{r fig.width=6, fig.height=5, fig.align='center'}
plot(perf)
```

We also want to know the log loss, MSE and AUC values, as well as other model metrics for the test data:

```{r }
h2o.logloss(perf)
h2o.mse(perf)
h2o.auc(perf)

head(h2o.metric(perf))
```

<br>

##### Deep learning with neural networks

```{r echo=TRUE, eval=FALSE}
hyper_params <- list(
                     activation = c("Rectifier", "Maxout", "Tanh", "RectifierWithDropout", 
                                    "MaxoutWithDropout", "TanhWithDropout"), 
                     hidden = list(c(5, 5, 5, 5, 5), c(10, 10, 10, 10), c(50, 50, 50)),
                     epochs = c(50, 100, 200),
                     l1 = c(0, 0.00001, 0.0001), 
                     l2 = c(0, 0.00001, 0.0001),
                     rate = c(0, 01, 0.005, 0.001),
                     rate_annealing = c(1e-8, 1e-7, 1e-6),
                     rho = c(0.9,0.95,0.99,0.999),
                     epsilon = c(1e-10,1e-8,1e-6,1e-4),
                     input_dropout_ratio = c(0, 0.1, 0.2),
                     max_w2 = c(10, 100, 1000, 3.4028235e+38)
                     )
```

```{r echo=TRUE, eval=FALSE}
dl_grid <- h2o.grid(algorithm = "deeplearning", 
                    x = features,
                    y = response,
                    grid_id = "dl_grid",
                    training_frame = train,
                    validation_frame = valid,
                    nfolds = 25,                           
                    fold_assignment = "Stratified",
                    hyper_params = hyper_params,
                    search_criteria = search_criteria,
                    seed = 42
                    )
```

```{r echo=TRUE, eval=FALSE}
grid <- h2o.getGrid("dl_grid", sort_by = "auc", decreasing = TRUE)
  
model_ids <- grid@model_ids
best_model <- h2o.getModel(model_ids[[1]])
```

Because training can take a while, depending on how many samples, features, nodes and hidden layers you are training on, it is a good idea to save your model.

```{r echo=TRUE, eval=FALSE}
h2o.saveModel(best_model, path="models", force = TRUE)
```

We can then re-load the model again any time to check the model quality and make predictions on new data.

```{r}
dl_model <- h2o.loadModel("U:\\Github_blog\\Webinar\\Webinar_ML_for_disease\\models\\dl_grid_model_8")
```

```{r}
perf <- h2o.performance(best_model, test)
plot(perf)

h2o.confusionMatrix(best_model, test)
```

---

## Exercises

Try to run the analyses on the following datasets:

### Arrhythmia data

The [arrhythmia dataset](https://archive.ics.uci.edu/ml/datasets/Arrhythmia) from the UC Irvine Machine Learning repository contains 279 features from ECG heart rhythm diagnostics and one output column. I am not going to rename the feature columns because they are too many and the descriptions are too complex. Also, we don't need to know specifically which features we are looking at for building the models. For a description of each feature, see [https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.names](https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.names). The output column defines 16 classes: class 1 samples are from healthy ECGs, the remaining classes belong to different types of arrhythmia, with class 16 being all remaining arrhythmia cases that didn't fit into distinct classes.

```{r eval=FALSE}
arrhythmia <- read.table("datasets/arrhythmia.data.txt", sep = ",")
arrhythmia[arrhythmia == "?"] <- NA

# making sure, that all feature columns are numeric
arrhythmia[-280] <- lapply(arrhythmia[-280], as.character)
arrhythmia[-280] <- lapply(arrhythmia[-280], as.numeric)

#  renaming output column and converting to factor
colnames(arrhythmia)[280] <- "class"
arrhythmia$class <- as.factor(arrhythmia$class)

arrhythmia$diagnosis <- ifelse(arrhythmia$class == 1, "healthy", "arrhythmia")
arrhythmia$diagnosis <- as.factor(arrhythmia$diagnosis)
```

```{r echo=FALSE, eval=FALSE}
save(arrhythmia, file = "datasets/arrhythmia.RData")
```

```{r echo=FALSE}
load("datasets/arrhythmia.RData")
```

<br>

### Flu data

Among the many R packages, there is the [outbreaks](https://mran.microsoft.com/web/packages/outbreaks/outbreaks.pdf) package. It contains datasets on epidemics, on of which is from the 2013 outbreak of [influenza A H7N9](http://www.who.int/influenza/human_animal_interface/faq_H7N9/en/) in [China](http://www.who.int/influenza/human_animal_interface/influenza_h7n9/ChinaH7N9JointMissionReport2013u.pdf?ua=1), as analysed by Kucharski et al. (2014):

```{r, tidy=FALSE}
library(outbreaks)
data(fluH7N9_china_2013)

# convert ? to NAs
fluH7N9_china_2013$age[which(fluH7N9_china_2013$age == "?")] <- NA

# create a new column with case ID
fluH7N9_china_2013$case.ID <- paste("case", fluH7N9_china_2013$case.ID, sep = "_")

# preparing the data frame for modeling
# 
library(dplyr)

dataset <- fluH7N9_china_2013 %>%
  mutate(hospital = as.factor(ifelse(is.na(date_of_hospitalisation), 0, 1)),
         gender_f = as.factor(ifelse(gender == "f", 1, 0)),
         province_Jiangsu = as.factor(ifelse(province == "Jiangsu", 1, 0)),
         province_Shanghai = as.factor(ifelse(province == "Shanghai", 1, 0)),
         province_Zhejiang = as.factor(ifelse(province == "Zhejiang", 1, 0)),
         province_other = as.factor(ifelse(province == "Zhejiang" | province == "Jiangsu" | province == "Shanghai", 0, 1)),
         days_onset_to_outcome = as.numeric(as.character(gsub(" days", "",
                                      as.Date(as.character(date_of_outcome), format = "%Y-%m-%d") - 
                                        as.Date(as.character(date_of_onset), format = "%Y-%m-%d")))),
         days_onset_to_hospital = as.numeric(as.character(gsub(" days", "",
                                      as.Date(as.character(date_of_hospitalisation), format = "%Y-%m-%d") - 
                                        as.Date(as.character(date_of_onset), format = "%Y-%m-%d")))),
         age = as.numeric(as.character(age)),
         early_onset = as.factor(ifelse(date_of_onset < summary(fluH7N9_china_2013$date_of_onset)[[3]], 1, 0)),
         early_outcome = as.factor(ifelse(date_of_outcome < summary(fluH7N9_china_2013$date_of_outcome)[[3]], 1, 0))) %>%
  subset(select = -c(2:4, 6, 8))
rownames(dataset) <- dataset$case_id
dataset <- dataset[, -1]

# impute missing data

library(mice)

dataset_impute <- mice(dataset[, -1],  print = FALSE)

# recombine imputed data frame with the outcome column

dataset_complete <- merge(dataset[, 1, drop = FALSE], mice::complete(dataset_impute, 1), by = "row.names", all = TRUE)
rownames(dataset_complete) <- dataset_complete$Row.names
dataset_complete <- dataset_complete[, -1]
```

```{r echo=FALSE, eval=FALSE}
save(dataset_complete, file = "datasets/dataset_complete.RData")
```

```{r echo=FALSE}
load("datasets/dataset_complete.RData")
```

<br>

------------------

<br>

```{r }
sessionInfo()
```
